mode: play
team1: scripted
team2: scripted
human_player: false

environment:
  render_mode: "human"
  world_size: [1024, 1024]
  memory_size: 2
  max_ships: 8
  agent_dt: 0.04
  physics_dt: 0.02
  random_positioning: true
  random_speed: true
  random_initialization: true

agents:
  scripted:
    agent_type: scripted
    agent_config:
      max_shooting_range: 500.0
      angle_threshold: 5.0
      bullet_speed: 500.0
      target_radius: 10.0
      radius_multiplier: 1.5
      world_size: [1024, 1024]

components:
  renderer:
    component_type: "renderer"
    component_config:
      render_mode: "human"
      target_fps: 60

collect:
  teams: [scripted, scripted]
  total_episodes: 100000
  type_ratios:
    type1: 0.25
    type2: 0.25
    type3: 0.25
    type4: 0.25
  ship_count_ratios:
    "1v1": 0.25
    "2v2": 0.25
    "3v3": 0.25
    "4v4": 0.25
  output_dir: "data/bc_pretraining"
  num_workers: 12
  save_frequency: 256
  render_mode: "none"
  max_episode_length: 512
  random_action_prob: 0.05

train:
  run_collect: true
  run_bc: false
  run_world_model: true
  run_rl: false
  
  use_bc: false
  use_rl: false
  
  bc_data_path: data\bc_pretraining\20260117_160241\aggregated_data.h5
  
  model:
    transformer:
      token_dim: 12
      embed_dim: 128
      num_heads: 4
      num_layers: 8
      max_ships: 8
      num_actions: 6
      dropout: 0.1
      use_layer_norm: true

  bc:
    learning_rate: 0.001
    batch_size: 8192
    epochs: 50
    validation_split: 0.2
    early_stopping_patience: 10
    policy_weight: 1.0
    value_weight: 0.5

  rl:
    policy_type: "transformer" # or "world_model"
    context_len: 32 # FrameStack k (for world_model)
    aux_loss_coef: 1.0 # Weight for WM aux loss
    freeze_world_model: false # Whether to freeze WM weights
    pretrained_model_path: null
    n_envs: 8
    total_timesteps: 1000000
    learning_rate: 0.0003
    n_steps: 4096
    batch_size: 64
    n_epochs: 10
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    ent_coef: 0.01
    vf_coef: 0.5
    max_grad_norm: 0.5

world_model:
  embed_dim: 128
  n_layers: 6
  n_heads: 4
  context_len: 96
  n_ships: 8
  mask_ratio: 0.0
  noise_scale: 0.1
  input_noise_ratio: 1.0
  input_noise_scale: 0.01
  learning_rate: 3.2e-5
  epochs: 100000

  scheduler:
    type: "constant" # Options: "constant", "linear_range_test", "exponential_range_test"
    start_lr: 1e-7
    end_lr: 1e-1
    num_steps: 834 # Fixed steps for range test
    end_lr: 1e-0 # Increase to ensure divergence
  
  # Mixed Batch Training
  short_batch_size: 192
  long_batch_size: 48
  short_batch_len: 32
  long_batch_len: 128
  batch_ratio: 4
  
  rope_base: 10000.0
  rope_max_seq_len: 256
  
  num_workers: 4
  prefetch_factor: 2

  # Closed Loop Rollout (Scheduled Sampling)
  rollout:
    enabled: true
    start_epoch: 32
    max_len_start: 1
    max_len_end: 8
    ramp_epochs: 96