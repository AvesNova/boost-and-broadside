mode: play
team1: scripted
team2: scripted
human_player: false
seed: 42

wandb:
  enabled: true
  project: "boost-and-broadside"
  entity: null
  name: null
  group: null
  mode: "online"
  log_frequency: 50

profiler:
  enabled: false
  wait: 32
  warmup: 4
  active: 5
  repeat: 1
  schedule_type: "skip_first"

environment:
  render_mode: "human"
  world_size: [1024, 1024]
  memory_size: 2
  max_ships: 8
  agent_dt: 0.04
  physics_dt: 0.02
  random_positioning: true
  random_speed: true
  random_initialization: true
  backend: "cpu"

agents:
  scripted:
    agent_type: scripted
    agent_config:
      max_shooting_range: 500.0
      angle_threshold: 5.0
      bullet_speed: 500.0
      target_radius: 10.0
      radius_multiplier: 1.5
      world_size: [1024, 1024]

components:
  renderer:
    component_type: "renderer"
    component_config:
      render_mode: "human"
      target_fps: 60

collect:
  teams: [scripted, scripted]
  total_episodes: 100000
  type_ratios:
    type1: 0.25
    type2: 0.25
    type3: 0.25
    type4: 0.25
  ship_count_ratios:
    "1v1": 0.25
    "2v2": 0.25
    "3v3": 0.25
    "4v4": 0.25
  output_dir: "data/bc_pretraining"
  num_workers: 12
  save_frequency: 256
  render_mode: "none"
  max_episode_length: 512
  random_action_prob: 0.05
  min_skill: 0.1
  max_skill: 1.0
  expert_ratio: 0.50 # Fraction of environments that are pure Expert vs Expert
  random_dist: "beta"
  
  massive:
    num_envs: 2048
    steps: 33554432
    batch_size: 2048
    buffer_steps: 512

train:
  run_collect: true
  run_bc: false
  run_world_model: true
  run_rl: false
  
  amp: true # Use Automatic Mixed Precision (bfloat16) for modern GPUs
  
  use_bc: false
  use_rl: false
  
  bc_data_path: data/bc_pretraining/aggregated_data.h5
  
  model:
    transformer:
      token_dim: 9
      embed_dim: 128
      num_heads: 4
      num_layers: 8
      max_ships: 8
      num_actions: 6
      dropout: 0.1
      use_layer_norm: true

  bc:
    learning_rate: 0.001
    batch_size: 8192
    epochs: 50
    validation_split: 0.2
    early_stopping_patience: 10
    policy_weight: 1.0
    value_weight: 0.5

  rl:
    policy_type: "transformer" # or "world_model"
    context_len: 32 # FrameStack k (for world_model)
    aux_loss_coef: 1.0 # Weight for WM aux loss
    freeze_world_model: false # Whether to freeze WM weights
    pretrained_model_path: null
    n_envs: 8
    total_timesteps: 1000000
    learning_rate: 0.0003
    n_steps: 4096
    batch_size: 64
    n_epochs: 10
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    ent_coef: 0.01
    vf_coef: 0.5
    max_grad_norm: 0.5

world_model:
  embed_dim: 256
  n_layers: 6
  n_heads: 4
  learning_rate: 3.5e-5
  epochs: 1000
  lambda_state: 1.0
  # Split Action Lambdas
  lambda_power: 0.05
  lambda_turn: 0.05
  lambda_shoot: 0.05
  lambda_relational: 0.1
  lambda_value: 0.5
  lambda_reward: 10.0

  curriculum:
    enabled: false
    min_skill_start: 0.9 # Start with only expert data
    min_skill_end: 0.0   # End with all data
    decay_steps: 1000    # Steps to reach min_skill_end (or use fraction of total steps)

  entropy:
    lambda_start: 0.00
    lambda_end: 0.00
    decay_steps: 10000

  loss:
    use_focal_loss: true
    gamma_start: 0.0
    gamma_end: 0.0
    decay_steps: 30000
    weighted_loss_power: 1.0
    weighted_loss_cap: 5.0

  scheduler:
    type: "warmup_constant"
    
    warmup:
      steps: 2048
      start_lr: 1e-7
      
    range_test:
      steps: 125
      start_lr: 1e-7
      end_lr: 1.0
  
  # Training Parameters
  batch_size: 24
  seq_len: 256
  gradient_accumulation_steps: 8
  use_sobol: false
  
  num_workers: 4
  prefetch_factor: 4
  swa_start_epoch: 2

  # Closed Loop Rollout (Scheduled Sampling)
  rollout:
    enabled: false
    start_epoch: 32
    max_len_start: 1
    max_len_end: 8
    ramp_epochs: 96